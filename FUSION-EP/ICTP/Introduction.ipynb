{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbb1cdcf",
   "metadata": {},
   "source": [
    "## Intro to ML in NP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f413643",
   "metadata": {},
   "source": [
    "Bayesian NNs. \n",
    "\n",
    "\n",
    "Nuclear structure reactions, what is a proton, fundamental particles -- all with AI specialization.\n",
    "\n",
    "Fusion is just one application. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f8e277",
   "metadata": {},
   "source": [
    "### Alpha - Davidson College | LINAC\n",
    "\n",
    "Active target time Projection Chamber | How a 3D reaction looks like. \n",
    "\n",
    "### Model Validation\n",
    "\n",
    "How well, a model generalises from the unseen data. Talk about the training and validation loss; plot: loss vs validation. **Loss curves**: they are extremely important to tell is we are doing good with the data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbf64ff",
   "metadata": {},
   "source": [
    "### Paper: approximation capabilities of multilayer feedforward networks. \n",
    "\n",
    "This paper proves that this is a general validation to NNs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67b4237",
   "metadata": {},
   "source": [
    "### Computational Graphs\n",
    "\n",
    "A computational graph is **not** a NN, nor implies ML. \n",
    "\n",
    "**Forward propagation** graph, for a linear equation. \n",
    "\n",
    "NNs, choose random values for the weigths. \n",
    "\n",
    "**Backpropagation** to update the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d2e99d",
   "metadata": {},
   "source": [
    "# Observations for my Thesis\n",
    "\n",
    "You really need to spend some time to properly introduce the Loss function. \n",
    "\n",
    "Testing for generalisability.\n",
    "\n",
    "* Common loss functions: the choose of loss function tells you what you are prediciting for; throught training you are providing samples.\n",
    "    - Mean absolute error (you will predict the median)\n",
    "    - Mean squared erros ()\n",
    "    - Quantile loss (to produce samples from the quantile function)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54649a2d",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "To update the weights for the $x$ node \n",
    "\n",
    "$$\n",
    "    w_x = w_x -\\eta^* \\cdot \\partial_\\hat{f} J \\cdot \\partial_{q_x} \\hat{f} \\cdot \\partial_{w_x} q_x\n",
    "$$\n",
    "\n",
    "$\\eta^*$ Hyperparameters\n",
    "\n",
    "--- \n",
    "\n",
    "How would you represent the sigmoid fuction for logistic regression in a computational graph? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca8b483",
   "metadata": {},
   "source": [
    "# On NNs\n",
    "\n",
    "When using the output as sigmoid, it does not necessearily implies it represents a probability because of how the algorithm has been trained. I think it is okay to see it as a probability in GPC because of the concept itself. \n",
    "\n",
    "NNs does not imply a previous structure on the data, so I think it is incorrect to interpret the output as a probability because the distribution needs a probability density distribution; hence, it would require a form. (?)\n",
    "\n",
    "**GET THE IMAGE FOR BACKPROPAGATION FOR A LINEAR REGRESSOR/ NODE**\n",
    "\n",
    "---\n",
    "\n",
    "**Libraries**\n",
    "\n",
    "Keras built from TensorFlow, coming out from google.\n",
    "\n",
    "PyTorch coming out from Facebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e641a3",
   "metadata": {},
   "source": [
    "# Can we model stochastic processes?\n",
    "\n",
    "Not the standard linear regression introduction\n",
    "\n",
    "**JET SIMULATION AND CORRECTION**\n",
    "\n",
    "How particles interact with detectors is a stochastic process. Particle Physicists what to model that stochasticiy as accurate as possible. \n",
    "\n",
    "---\n",
    "\n",
    "**IMPLICIT QUANTILE NETWORKS** Statistics Review\n",
    "\n",
    "**data** $\\rightarrow$ **CDF** (normalization/ cumulative distribution function -- inverse of quantile function) $\\rightarrow$ reversing the axis $\\rightarrow$ quantile function (why? because the output is a value coming from a distribution). \n",
    "\n",
    "It looks like a construction of a sigmoid fucntion but for multiclass problems (?)\n",
    "\n",
    "* Quantile functions for NNs\n",
    "\n",
    "* One considers the joint probability from the quantile functions\n",
    "\n",
    "$$\n",
    "    p(y^(1),\\dots, y^(n)|X) = Bayes\\:\\: theorem\n",
    "$$\n",
    "\n",
    "p(A,B,C,D) = p(A|D).p(B|A,D).p(C|A,B,D) \n",
    "\n",
    "\n",
    "Deep NNs are horribly parametrized, above 1000 parameters. However, the goal is to test generisability, not fit or create a model (?). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee8933",
   "metadata": {},
   "source": [
    "### QUESTIONs\n",
    "\n",
    "* Why is it necessary for a loss function to only take positive values? \n",
    "    - Assumptions are being done when one chooses the loss function; there are some of them that are not positive defined. Some of them can be minimized less than zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c7316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
