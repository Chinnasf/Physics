{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/alpha-davidson/ICTP-Citizen-Science-2023/blob/main/Activity1_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_-Q8naoDk36"
   },
   "source": [
    "# Neural Networks in TensorFlow's Keras\n",
    "\n",
    "\n",
    "We will use a dense neural network in Keras to solve a simple regression problem. \n",
    "## Learning Task\n",
    "We will construct a dense neural network to **predict the invariant mass of a particle** from its energy, momentum, (and charge).\n",
    "\n",
    "*Note that this task does not require machine learning. We choose a task with a known mapping to help us create, debug, and tune our first neural network.*\n",
    "\n",
    "## Dataset\n",
    "This dataset is a collection of simulated particle events from [Pythia](http://home.thep.lu.se/~torbjorn/Pythia.html). The dataset is a 2D array where each row represents one event from an $e^{-} + p$ (electron-proton) collision. This dataset is comprised _only_ of events where exactly 16 particles are produced from an electron-proton collision. Each particle contains $(p_x,p_y,p_z,E,q)$. Each event is therefore represented by 80 numbers. \n",
    "\n",
    "**Advanced activity:** There are more interesting event-wise learning tasks using this dataset. Consider crafting your own learning task and target for this data.\n",
    "\n",
    "\n",
    "\n",
    "## Computational Notes\n",
    "\n",
    "If this is your first time in a Jupyter-like environment, please read the following carefully:\n",
    "\n",
    " - You are in an active kernel\n",
    " - Run each cell with `Shift + Enter`\n",
    " - You must execute the cells in the order that you want the code to run\n",
    " - In Colab, `Runtime`$\\rightarrow$`Change runtime type` allows you to utilize GPUs and TPUs. They are unnecessary here, but will become vital in later exercises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LfL6KNLSDk3-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-23 17:20:06.009325: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-23 17:20:06.041826: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-23 17:20:06.042374: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-23 17:20:06.545393: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# import the packages we will be using\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pylab as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xTx3KTNYGyQu"
   },
   "outputs": [],
   "source": [
    "# Import data from github. Note: in colab, go to Files and refresh to see file.\n",
    "# Here I use Linux commands within the notebook to pull the data file and rename it.\n",
    "\n",
    "# If you are not in an environment that supports the following commands, \n",
    "#     paste the url below to download the dataset onto your local machine.\n",
    "\n",
    "\"\"\"!wget https://github.com/NuclearTalent/MachineLearningECT/blob/master/doc/pub/Day6/data/homogenous-16-particle-events-energy.npy?raw=true\n",
    "!mv homogenous-16-particle-events-energy.npy?raw=true particle-events.npy\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8-GfZs9bI7Km"
   },
   "outputs": [],
   "source": [
    "# now we load the data file, which is a numpy array\n",
    "events = np.load(\"particle-events.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35916, 80)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [f\"px{i}|py{i}|pz{i}|E{i}|q{i}\" for i in range(1,17)]\n",
    "features = []\n",
    "for col in col_names:\n",
    "    features = features + col.split(\"|\")\n",
    "df.columns = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>px1</th>\n",
       "      <th>py1</th>\n",
       "      <th>pz1</th>\n",
       "      <th>E1</th>\n",
       "      <th>q1</th>\n",
       "      <th>px2</th>\n",
       "      <th>py2</th>\n",
       "      <th>pz2</th>\n",
       "      <th>E2</th>\n",
       "      <th>q2</th>\n",
       "      <th>...</th>\n",
       "      <th>px15</th>\n",
       "      <th>py15</th>\n",
       "      <th>pz15</th>\n",
       "      <th>E15</th>\n",
       "      <th>q15</th>\n",
       "      <th>px16</th>\n",
       "      <th>py16</th>\n",
       "      <th>pz16</th>\n",
       "      <th>E16</th>\n",
       "      <th>q16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.492933</td>\n",
       "      <td>-0.469448</td>\n",
       "      <td>-2.228710</td>\n",
       "      <td>2.334520</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.380699</td>\n",
       "      <td>0.859243</td>\n",
       "      <td>0.125686</td>\n",
       "      <td>0.958388</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060035</td>\n",
       "      <td>0.198486</td>\n",
       "      <td>0.387630</td>\n",
       "      <td>0.461235</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.182514</td>\n",
       "      <td>0.156148</td>\n",
       "      <td>-1.763670</td>\n",
       "      <td>2.01211</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.021128</td>\n",
       "      <td>0.513518</td>\n",
       "      <td>0.056984</td>\n",
       "      <td>0.535606</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.357259</td>\n",
       "      <td>0.326994</td>\n",
       "      <td>-0.533131</td>\n",
       "      <td>0.733667</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.329790</td>\n",
       "      <td>-0.872136</td>\n",
       "      <td>4.813310</td>\n",
       "      <td>5.919080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.238305</td>\n",
       "      <td>0.223850</td>\n",
       "      <td>-0.542793</td>\n",
       "      <td>1.13220</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.238883</td>\n",
       "      <td>0.010763</td>\n",
       "      <td>-1.438530</td>\n",
       "      <td>1.464940</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.166321</td>\n",
       "      <td>0.511802</td>\n",
       "      <td>0.878539</td>\n",
       "      <td>1.039670</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.371150</td>\n",
       "      <td>0.149630</td>\n",
       "      <td>12.704600</td>\n",
       "      <td>13.145800</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.033907</td>\n",
       "      <td>0.150283</td>\n",
       "      <td>-3.653550</td>\n",
       "      <td>3.77525</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.100541</td>\n",
       "      <td>0.042173</td>\n",
       "      <td>0.535900</td>\n",
       "      <td>0.564407</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.481310</td>\n",
       "      <td>-0.186366</td>\n",
       "      <td>0.788642</td>\n",
       "      <td>0.952800</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.132850</td>\n",
       "      <td>-0.677265</td>\n",
       "      <td>3.723820</td>\n",
       "      <td>4.346730</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.321742</td>\n",
       "      <td>-0.034143</td>\n",
       "      <td>-1.928730</td>\n",
       "      <td>2.16911</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.701960</td>\n",
       "      <td>4.534330</td>\n",
       "      <td>5.796190</td>\n",
       "      <td>8.238930</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.250877</td>\n",
       "      <td>0.350684</td>\n",
       "      <td>-0.728698</td>\n",
       "      <td>0.858137</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125843</td>\n",
       "      <td>0.094416</td>\n",
       "      <td>-0.093951</td>\n",
       "      <td>0.230342</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.624977</td>\n",
       "      <td>-0.087251</td>\n",
       "      <td>-2.548230</td>\n",
       "      <td>2.78784</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35911</th>\n",
       "      <td>-1.333440</td>\n",
       "      <td>1.059830</td>\n",
       "      <td>1.372490</td>\n",
       "      <td>2.191920</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.554874</td>\n",
       "      <td>0.015482</td>\n",
       "      <td>0.105968</td>\n",
       "      <td>0.582094</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.181712</td>\n",
       "      <td>0.312801</td>\n",
       "      <td>-0.281528</td>\n",
       "      <td>0.479168</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.743300</td>\n",
       "      <td>0.552026</td>\n",
       "      <td>-0.433851</td>\n",
       "      <td>1.38773</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35912</th>\n",
       "      <td>-0.337079</td>\n",
       "      <td>-0.092994</td>\n",
       "      <td>-1.016500</td>\n",
       "      <td>1.083980</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.078552</td>\n",
       "      <td>0.086005</td>\n",
       "      <td>2.134450</td>\n",
       "      <td>2.142170</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117765</td>\n",
       "      <td>-0.366370</td>\n",
       "      <td>-0.163362</td>\n",
       "      <td>0.440752</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.325950</td>\n",
       "      <td>-0.327742</td>\n",
       "      <td>-1.620970</td>\n",
       "      <td>1.92913</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35913</th>\n",
       "      <td>-0.494778</td>\n",
       "      <td>-0.198240</td>\n",
       "      <td>1.594910</td>\n",
       "      <td>1.687400</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.005706</td>\n",
       "      <td>0.441082</td>\n",
       "      <td>-0.332050</td>\n",
       "      <td>0.569493</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274823</td>\n",
       "      <td>-0.141276</td>\n",
       "      <td>-0.151151</td>\n",
       "      <td>0.371232</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.776436</td>\n",
       "      <td>-0.033002</td>\n",
       "      <td>-3.395280</td>\n",
       "      <td>3.60725</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35914</th>\n",
       "      <td>-0.550329</td>\n",
       "      <td>0.533369</td>\n",
       "      <td>0.111079</td>\n",
       "      <td>0.786869</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.021931</td>\n",
       "      <td>0.324879</td>\n",
       "      <td>0.326723</td>\n",
       "      <td>0.481928</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.393085</td>\n",
       "      <td>0.514364</td>\n",
       "      <td>-0.484226</td>\n",
       "      <td>0.820390</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.382212</td>\n",
       "      <td>-0.248855</td>\n",
       "      <td>-0.179829</td>\n",
       "      <td>1.05863</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35915</th>\n",
       "      <td>-0.444629</td>\n",
       "      <td>0.980696</td>\n",
       "      <td>0.676546</td>\n",
       "      <td>1.279320</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.391059</td>\n",
       "      <td>0.372074</td>\n",
       "      <td>0.552607</td>\n",
       "      <td>0.784997</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038891</td>\n",
       "      <td>-0.069566</td>\n",
       "      <td>0.337187</td>\n",
       "      <td>0.373533</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.283360</td>\n",
       "      <td>0.373694</td>\n",
       "      <td>-2.622290</td>\n",
       "      <td>2.82430</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35916 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            px1       py1       pz1        E1   q1       px2       py2  \\\n",
       "0     -0.492933 -0.469448 -2.228710  2.334520 -1.0 -0.380699  0.859243   \n",
       "1     -0.021128  0.513518  0.056984  0.535606 -1.0  0.357259  0.326994   \n",
       "2     -0.238883  0.010763 -1.438530  1.464940 -1.0  0.166321  0.511802   \n",
       "3      0.100541  0.042173  0.535900  0.564407 -1.0  0.481310 -0.186366   \n",
       "4     -3.701960  4.534330  5.796190  8.238930 -1.0 -0.250877  0.350684   \n",
       "...         ...       ...       ...       ...  ...       ...       ...   \n",
       "35911 -1.333440  1.059830  1.372490  2.191920 -1.0 -0.554874  0.015482   \n",
       "35912 -0.337079 -0.092994 -1.016500  1.083980 -1.0 -0.078552  0.086005   \n",
       "35913 -0.494778 -0.198240  1.594910  1.687400 -1.0 -0.005706  0.441082   \n",
       "35914 -0.550329  0.533369  0.111079  0.786869 -1.0  0.021931  0.324879   \n",
       "35915 -0.444629  0.980696  0.676546  1.279320 -1.0 -0.391059  0.372074   \n",
       "\n",
       "            pz2        E2   q2  ...      px15      py15       pz15        E15  \\\n",
       "0      0.125686  0.958388 -1.0  ...  0.060035  0.198486   0.387630   0.461235   \n",
       "1     -0.533131  0.733667 -1.0  ...  3.329790 -0.872136   4.813310   5.919080   \n",
       "2      0.878539  1.039670 -1.0  ...  3.371150  0.149630  12.704600  13.145800   \n",
       "3      0.788642  0.952800 -1.0  ...  2.132850 -0.677265   3.723820   4.346730   \n",
       "4     -0.728698  0.858137 -1.0  ...  0.125843  0.094416  -0.093951   0.230342   \n",
       "...         ...       ...  ...  ...       ...       ...        ...        ...   \n",
       "35911  0.105968  0.582094 -1.0  ... -0.181712  0.312801  -0.281528   0.479168   \n",
       "35912  2.134450  2.142170 -1.0  ...  0.117765 -0.366370  -0.163362   0.440752   \n",
       "35913 -0.332050  0.569493 -1.0  ...  0.274823 -0.141276  -0.151151   0.371232   \n",
       "35914  0.326723  0.481928 -1.0  ... -0.393085  0.514364  -0.484226   0.820390   \n",
       "35915  0.552607  0.784997 -1.0  ... -0.038891 -0.069566   0.337187   0.373533   \n",
       "\n",
       "       q15      px16      py16      pz16      E16  q16  \n",
       "0      1.0 -0.182514  0.156148 -1.763670  2.01211  1.0  \n",
       "1      1.0  0.238305  0.223850 -0.542793  1.13220  1.0  \n",
       "2      1.0 -0.033907  0.150283 -3.653550  3.77525  1.0  \n",
       "3      1.0 -0.321742 -0.034143 -1.928730  2.16911  1.0  \n",
       "4      1.0  0.624977 -0.087251 -2.548230  2.78784  1.0  \n",
       "...    ...       ...       ...       ...      ...  ...  \n",
       "35911  1.0 -0.743300  0.552026 -0.433851  1.38773  1.0  \n",
       "35912  1.0 -0.325950 -0.327742 -1.620970  1.92913  1.0  \n",
       "35913  1.0 -0.776436 -0.033002 -3.395280  3.60725  1.0  \n",
       "35914  1.0  0.382212 -0.248855 -0.179829  1.05863  1.0  \n",
       "35915  1.0 -0.283360  0.373694 -2.622290  2.82430  1.0  \n",
       "\n",
       "[35916 rows x 80 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8O8n_nwTDk3_"
   },
   "source": [
    "Recall that each row of this dataset is an entire event. We need each row to represent a training example, which is a single particle.\n",
    "\n",
    "Using `numpy`'s `reshape` method we can make each row represent one particle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "067guDXZDk4A"
   },
   "outputs": [],
   "source": [
    "# Here we rearrange the data within each of the events to isolate particles\n",
    "\n",
    "#print(\"events[0] =\\n\",events[0])\n",
    "evt_particles = np.reshape(events, (len(events), 16, 5))\n",
    "\n",
    "#print(\"\\nevt_particles[0] =\\n\", evt_particles[0])\n",
    "\n",
    "# Use another call of reshape to combine all events to have the appropriate shape\n",
    "# Complete me:\n",
    "#particles = \n",
    "\n",
    "#print(\"\\nparticles =\\n\",particles[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_YWTBO0Dk4B"
   },
   "source": [
    "These are our training data inputs, but we also must provide the **targets**, which are the **invariant masses of each particle**. This is a straightforward computation.\n",
    "\n",
    "We choose units where $c = 1$:\n",
    "$$m^2=E^2-||\\textbf{p}||^2$$ \n",
    "where $m, E$, and $\\textbf{p}$ are all in GeV. This is equivalent to \n",
    "$$m = \\sqrt{E^2 - (p_x^2 + p_y^2 + p_z^2)}$$\n",
    "\n",
    "**Create an array of your target values.**\n",
    "Due to insufficient precision, some $m^2$ values for massless particles will come out very slightly negative. These should be treated as zero to avoid `NaN`. I used the `maximum` method from `numpy` to handle this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ap3EizhuDk4B"
   },
   "outputs": [],
   "source": [
    "# Complete me:\n",
    "masses = pd.DataFrame()\n",
    "for i in range(16):\n",
    "    px = np.array(df[f\"px{i+1}\"]); py = np.array(df[f\"py{i+1}\"]); pz = np.array(df[f\"pz{i+1}\"])\n",
    "    E  = np.array(df[f\"E{i+1}\"] )\n",
    "    arg = E**2 - (px**2 + py**2 + pz**2)\n",
    "    masses[f\"m{i+1}\"] = np.sqrt( np.maximum(np.zeros(len(arg)), arg) )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightarrow$ Proton is the heaviest particle created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m1</th>\n",
       "      <th>m2</th>\n",
       "      <th>m3</th>\n",
       "      <th>m4</th>\n",
       "      <th>m5</th>\n",
       "      <th>m6</th>\n",
       "      <th>m7</th>\n",
       "      <th>m8</th>\n",
       "      <th>m9</th>\n",
       "      <th>m10</th>\n",
       "      <th>m11</th>\n",
       "      <th>m12</th>\n",
       "      <th>m13</th>\n",
       "      <th>m14</th>\n",
       "      <th>m15</th>\n",
       "      <th>m16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35916.000000</td>\n",
       "      <td>35916.000000</td>\n",
       "      <td>35916.000000</td>\n",
       "      <td>35916.000000</td>\n",
       "      <td>35916.000000</td>\n",
       "      <td>35916.000000</td>\n",
       "      <td>35916.000000</td>\n",
       "      <td>35916.000000</td>\n",
       "      <td>35916.000000</td>\n",
       "      <td>35916.000000</td>\n",
       "      <td>35916.000000</td>\n",
       "      <td>35916.000000</td>\n",
       "      <td>35916.000000</td>\n",
       "      <td>35916.000000</td>\n",
       "      <td>35916.000000</td>\n",
       "      <td>35916.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.139570</td>\n",
       "      <td>0.139569</td>\n",
       "      <td>0.139571</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>0.139565</td>\n",
       "      <td>0.139571</td>\n",
       "      <td>0.139571</td>\n",
       "      <td>0.938270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.016553</td>\n",
       "      <td>0.003334</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.001648</td>\n",
       "      <td>0.003257</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.129948</td>\n",
       "      <td>0.130457</td>\n",
       "      <td>0.129184</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126122</td>\n",
       "      <td>0.131046</td>\n",
       "      <td>0.128824</td>\n",
       "      <td>0.938208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.139565</td>\n",
       "      <td>0.139568</td>\n",
       "      <td>0.139565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139556</td>\n",
       "      <td>0.139566</td>\n",
       "      <td>0.139556</td>\n",
       "      <td>0.938264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.139570</td>\n",
       "      <td>0.139570</td>\n",
       "      <td>0.139570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.139570</td>\n",
       "      <td>0.139570</td>\n",
       "      <td>0.139570</td>\n",
       "      <td>0.938270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.139575</td>\n",
       "      <td>0.139572</td>\n",
       "      <td>0.139575</td>\n",
       "      <td>0.028929</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.139586</td>\n",
       "      <td>0.139574</td>\n",
       "      <td>0.139584</td>\n",
       "      <td>0.938276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.148373</td>\n",
       "      <td>0.146542</td>\n",
       "      <td>0.147937</td>\n",
       "      <td>0.060916</td>\n",
       "      <td>0.049075</td>\n",
       "      <td>0.048046</td>\n",
       "      <td>0.046212</td>\n",
       "      <td>0.033206</td>\n",
       "      <td>0.037981</td>\n",
       "      <td>0.032007</td>\n",
       "      <td>0.040221</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>0.150444</td>\n",
       "      <td>0.148468</td>\n",
       "      <td>0.149858</td>\n",
       "      <td>0.938331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 m1            m2            m3            m4            m5  \\\n",
       "count  35916.000000  35916.000000  35916.000000  35916.000000  35916.000000   \n",
       "mean       0.139570      0.139569      0.139571      0.014286      0.001352   \n",
       "std        0.000490      0.000159      0.000485      0.016553      0.003334   \n",
       "min        0.129948      0.130457      0.129184      0.000000      0.000000   \n",
       "25%        0.139565      0.139568      0.139565      0.000000      0.000000   \n",
       "50%        0.139570      0.139570      0.139570      0.000000      0.000000   \n",
       "75%        0.139575      0.139572      0.139575      0.028929      0.000848   \n",
       "max        0.148373      0.146542      0.147937      0.060916      0.049075   \n",
       "\n",
       "                 m6            m7            m8            m9           m10  \\\n",
       "count  35916.000000  35916.000000  35916.000000  35916.000000  35916.000000   \n",
       "mean       0.000749      0.000493      0.000391      0.000395      0.000490   \n",
       "std        0.001630      0.001183      0.000946      0.000979      0.001164   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000007      0.000000      0.000000   \n",
       "75%        0.000546      0.000448      0.000403      0.000406      0.000435   \n",
       "max        0.048046      0.046212      0.033206      0.037981      0.032007   \n",
       "\n",
       "                m11           m12           m13           m14           m15  \\\n",
       "count  35916.000000  35916.000000  35916.000000  35916.000000  35916.000000   \n",
       "mean       0.000746      0.001369      0.139565      0.139571      0.139571   \n",
       "std        0.001648      0.003257      0.000846      0.000264      0.000820   \n",
       "min        0.000000      0.000000      0.126122      0.131046      0.128824   \n",
       "25%        0.000000      0.000000      0.139556      0.139566      0.139556   \n",
       "50%        0.000000      0.000007      0.139570      0.139570      0.139570   \n",
       "75%        0.000547      0.000918      0.139586      0.139574      0.139584   \n",
       "max        0.040221      0.047000      0.150444      0.148468      0.149858   \n",
       "\n",
       "                m16  \n",
       "count  35916.000000  \n",
       "mean       0.938270  \n",
       "std        0.000012  \n",
       "min        0.938208  \n",
       "25%        0.938264  \n",
       "50%        0.938270  \n",
       "75%        0.938276  \n",
       "max        0.938331  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masses.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kprt9pbPDk4D"
   },
   "source": [
    "There are several hundred thousand datapoints in this dataset which is overkill for this simple example. Create a test dataset with just 1000 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nZkF1erNDk4E"
   },
   "outputs": [],
   "source": [
    "#Slicing allows you select a subset of an array.\n",
    "#This can be done like this: smallerArray = largerArray[:100]\n",
    "\n",
    "complete_data = pd.concat([df,masses], axis=1, join=\"inner\")\n",
    "data_subset = complete_data.sample(n=1000, replace=False, weights=None, random_state=71)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VYyii0yDk4F"
   },
   "source": [
    "Next, make a histogram of the target data to make sure that we are seeing masses of real particles. As this data has limited precision, this will not resolve electrons very well, but protons, pions, and massless particles should be clearly visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xHL4sU3ZDk4G"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data_subset[masses.columns].stack().tolist(),bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-MXhgPQDk4H"
   },
   "source": [
    "Now we can build and train the first neural network. We will start with a network with on hidden layer and 5 neurons, and ReLU activation. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "d_MsoqFvDk4H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 20)                1620      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,641\n",
      "Trainable params: 1,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build our model\n",
    "model = tf.keras.Sequential() #Define the model object\n",
    "model.add(tf.keras.layers.Dense(units=20, # Number of neurons in hidden layer \n",
    "                                input_shape=(len(features),), \n",
    "                                activation=\"relu\")\n",
    "         ) #Add a single hidden layer\n",
    "\n",
    "# Add the output layer yourself\n",
    "# No activation function assignment defaults to \"linear\"\n",
    "# Complete me:\n",
    "model.add(tf.keras.layers.Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "# prints a model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFb0E6GtSK7Z"
   },
   "source": [
    "To start, train for 30 epochs with a batch size of 128, an Adam optimizer with a learning rate of 0.1, using mean squared error loss. **This is not ideal.**\n",
    "\n",
    "Define your validation split to provide a set of data the model has not used to assess how the model is generalizing. This allows us to monitor overtraining. Setting `validation_split=0.2` meaning that $20\\%$ of the data will be used for validation. **This is not ideal.**\n",
    "\n",
    "Information on how to implement these features can be found here:\n",
    "<https://www.tensorflow.org/api_docs/python/tf/keras>.\n",
    "Check out Sequential underneath models and Dense under layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "hCp3EXCTDk4I"
   },
   "outputs": [],
   "source": [
    "optimize = tf.keras.optimizers.Adam(learning_rate=0.1) # Adam optimizer with a learning rate of 0.1\n",
    "loss_func = tf.keras.losses.MeanSquaredError() # choosing MSE\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer=optimize,loss=loss_func) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 80)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data_subset.drop(masses.columns, axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 16)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data_subset[masses.columns]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Zs_ReONMDk4I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 188.0746 - val_loss: 0.1400\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.1376 - val_loss: 0.1298\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.1427 - val_loss: 0.2336\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.2535 - val_loss: 0.2768\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2372 - val_loss: 0.2100\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.2209 - val_loss: 0.2526\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.2502 - val_loss: 0.2194\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.2094 - val_loss: 0.2005\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1958 - val_loss: 0.1898\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1784 - val_loss: 0.1624\n"
     ]
    }
   ],
   "source": [
    "results = model.fit(X, y, epochs=10, batch_size=256, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mcg4Qj1wZDxE"
   },
   "source": [
    "A learning curve is a clearer way to visualize the above training. This helps us determine over and underfitting, which, in turn, helps us tune our model and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "TrIo4jsqDk4J"
   },
   "outputs": [],
   "source": [
    "# helper function for easy plotting\n",
    "def plot_learning_curve(history):\n",
    "    plt.plot(history[\"loss\"], label=\"training loss\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"validation loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "RfhycMMXZahi"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# using the new function\n",
    "plot_learning_curve(results.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rot49hATDk4K"
   },
   "source": [
    "## Tuning\n",
    "The model is learning, but we can do better. \n",
    "\n",
    "**At this point, you can play with the above code to change hyperparameters and note changes to your results.**\n",
    "\n",
    "Alternatively, you can step through tuning this model below.\n",
    "\n",
    "## Walk-through tuning:\n",
    "Perhaps we did not have enough model parameters to accurately represent the mapping. **Remedy this by increasing the number of hidden neurons to 20.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "N3SfAb-mDk4L"
   },
   "outputs": [],
   "source": [
    "#Use the same code as in the previous cell\n",
    "#simply change the number of neurons in the hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQWoQKonDk4L"
   },
   "source": [
    "We see that we got little improvement here. Another hyperparameter to adjust is *batch size*, which is the number of training examples used to calculate the gradient on each step. While you may initially think that a higher batch size leads to faster or more accurate training, in practice this is not true. The \"noise\" that arises from using less training examples at each iteration can actually help find the global minimum of the loss function.\n",
    "(See here for more info: https://arxiv.org/pdf/1609.04836.pdf)\n",
    "\n",
    "Try decreasing the batch size to 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "UB9__PwjDk4M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0902 - val_loss: 0.0537\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0505 - val_loss: 0.0497\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0498 - val_loss: 0.0497\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0497 - val_loss: 0.0497\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0497 - val_loss: 0.0497\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0497 - val_loss: 0.0497\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0497 - val_loss: 0.0497\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0497 - val_loss: 0.0497\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0497 - val_loss: 0.0497\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0497 - val_loss: 0.0497\n"
     ]
    }
   ],
   "source": [
    "#Complete me:\n",
    "results = model.fit(X, y, epochs=10, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4QGMh3-Dk4M"
   },
   "source": [
    "This is starting to do better but has significant room for improvement.\n",
    "\n",
    "Another hyperparameter to tune is the *learning rate*. \n",
    "\n",
    " - If the learning rate is too high, we are taking too large of a step in the gradient descent at each iteration and will miss narrow minima in the loss function. \n",
    " - If the learning rate is too small, then we are not traveling far enough in each iteration and we will take far too long to reach a minimum. \n",
    "\n",
    "Perhaps the learning rate is too high and the network can't fine tune. Try decreasing the learning rate to 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "s851AM6GDk4N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0498 - val_loss: 0.0497\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0497 - val_loss: 0.0497\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0497 - val_loss: 0.0497\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0497 - val_loss: 0.0497\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0497 - val_loss: 0.0497\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0497 - val_loss: 0.0497\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0497 - val_loss: 0.0497\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0497 - val_loss: 0.0497\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0497 - val_loss: 0.0497\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0497 - val_loss: 0.0497\n"
     ]
    }
   ],
   "source": [
    "# Complete me:\n",
    "\n",
    "optimize = tf.keras.optimizers.Adam(learning_rate=0.001) # Adam optimizer with a learning rate of 0.001\n",
    "model.compile(optimizer=optimize,loss=loss_func) \n",
    "results = model.fit(X, y, epochs=10, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yh748SgrDk4O"
   },
   "source": [
    "This is not really that much better, but now there is evidence of *overtraining* or *overfitting* -- the training loss is so much lower than the validation loss. \n",
    "\n",
    "A common fix to this is adding *dropout layers*. Try adding a dropout layer with dropout rate of 0.5. <https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout>\n",
    "\n",
    "You can also try batch normalization: <https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "XO_Ou80xDk4O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0497 - val_loss: 0.0497\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0497 - val_loss: 0.0497\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0497 - val_loss: 0.0497\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0497 - val_loss: 0.0497\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0497 - val_loss: 0.0497\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0497 - val_loss: 0.0497\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0497 - val_loss: 0.0497\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0497 - val_loss: 0.0497\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0497 - val_loss: 0.0497\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0497 - val_loss: 0.0497\n"
     ]
    }
   ],
   "source": [
    "#Dropout layers are located under tf.keras.layers. \n",
    "#They take the dropout rate as their only argument.\n",
    "#BatchNormalization layers are also under tf.keras.layers, and in the simplest use case, take no arguments\n",
    "\n",
    "#Complete me:\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "results = model.fit(X, y, epochs=10, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Owd0k7kEDk4P"
   },
   "source": [
    "This clearly stopped the overtraining problem, but it still isn't training well. Now, try training on the full dataset with a more reasonable validation split of 0.2. Use a single hidden layer with 20 neurons, a learning rate of 0.001, and a batch size of 256. Just run it for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "iStR0aT5Dk4P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 3.9785 - val_loss: 0.8142\n",
      "Epoch 2/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.5114 - val_loss: 0.3315\n",
      "Epoch 3/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2632 - val_loss: 0.2170\n",
      "Epoch 4/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1898 - val_loss: 0.1695\n",
      "Epoch 5/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1537 - val_loss: 0.1415\n",
      "Epoch 6/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1316 - val_loss: 0.1240\n",
      "Epoch 7/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1165 - val_loss: 0.1115\n",
      "Epoch 8/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1057 - val_loss: 0.1026\n",
      "Epoch 9/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0976 - val_loss: 0.0953\n",
      "Epoch 10/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0911 - val_loss: 0.0894\n"
     ]
    }
   ],
   "source": [
    "#Complete me:\n",
    "\n",
    "model = tf.keras.Sequential() #Define the model object\n",
    "model.add(tf.keras.layers.Dense(units=20, # Number of neurons in hidden layer \n",
    "                                input_shape=(len(features),), \n",
    "                                activation=\"relu\")\n",
    "         ) #Add a single hidden layer\n",
    "model.add(tf.keras.layers.Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "optimize = tf.keras.optimizers.Adam(learning_rate=0.001) # Adam optimizer with a learning rate of 0.001\n",
    "loss_func = tf.keras.losses.MeanSquaredError() # choosing MSE\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer=optimize,loss=loss_func) \n",
    "\n",
    "X_ = complete_data[features]\n",
    "y_ = complete_data[masses.columns]\n",
    "\n",
    "results = model.fit(X_, y_, epochs=10, batch_size=256, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vkQ1kX2Dk4Q"
   },
   "source": [
    "This clearly resulted in a significant improvement and shows how important having a large enough dataset is. Moving on to the choice in activation functions, ReLU is not the only available choice, although it is one of the most popular ones currently. Try training a network using a sigmoid or tanh activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "B3qtiBmiDk4Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.4915 - val_loss: 0.1445\n",
      "Epoch 2/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1193 - val_loss: 0.1012\n",
      "Epoch 3/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0903 - val_loss: 0.0838\n",
      "Epoch 4/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0773 - val_loss: 0.0741\n",
      "Epoch 5/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0699 - val_loss: 0.0680\n",
      "Epoch 6/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0649 - val_loss: 0.0636\n",
      "Epoch 7/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0613 - val_loss: 0.0605\n",
      "Epoch 8/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0587 - val_loss: 0.0582\n",
      "Epoch 9/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0567 - val_loss: 0.0564\n",
      "Epoch 10/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0552 - val_loss: 0.0551\n"
     ]
    }
   ],
   "source": [
    "#Simply change relu to sigmoid or tanh to change the activation function\n",
    "\n",
    "#Complete me:\n",
    "\n",
    "model = tf.keras.Sequential() #Define the model object\n",
    "model.add(tf.keras.layers.Dense(units=20, # Number of neurons in hidden layer \n",
    "                                input_shape=(len(features),), \n",
    "                                activation=\"tanh\")\n",
    "         ) #Add a single hidden layer\n",
    "model.add(tf.keras.layers.Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "optimize = tf.keras.optimizers.Adam(learning_rate=0.001) # Adam optimizer with a learning rate of 0.001\n",
    "loss_func = tf.keras.losses.MeanSquaredError() # choosing MSE\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer=optimize,loss=loss_func) \n",
    "\n",
    "X_ = complete_data[features]\n",
    "y_ = complete_data[masses.columns]\n",
    "\n",
    "results = model.fit(X_, y_, epochs=10, batch_size=256, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRlAj8KFDk4R"
   },
   "source": [
    "Next, try adding 2 new hidden layers to the network. Use the ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "oJlPFnsjDk4R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "113/113 [==============================] - 1s 2ms/step - loss: 0.4924 - val_loss: 0.0622\n",
      "Epoch 2/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0578 - val_loss: 0.0549\n",
      "Epoch 3/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0538 - val_loss: 0.0529\n",
      "Epoch 4/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0524 - val_loss: 0.0520\n",
      "Epoch 5/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0517 - val_loss: 0.0513\n",
      "Epoch 6/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0511 - val_loss: 0.0509\n",
      "Epoch 7/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0508 - val_loss: 0.0506\n",
      "Epoch 8/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0505 - val_loss: 0.0505\n",
      "Epoch 9/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0504 - val_loss: 0.0503\n",
      "Epoch 10/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0503 - val_loss: 0.0502\n"
     ]
    }
   ],
   "source": [
    "#Complete me: COMPLETE MODEL\n",
    "model = tf.keras.Sequential() #Define the model object\n",
    "model.add(tf.keras.layers.Dense(units=20, # Number of neurons in hidden layer \n",
    "                                input_shape=(len(features),), \n",
    "                                activation=\"ReLU\")\n",
    "         ) #Add a single hidden layer 1\n",
    "model.add(tf.keras.layers.Dense(units=20, # Number of neurons in hidden layer \n",
    "                                input_shape=(len(features),), \n",
    "                                activation=\"ReLU\")\n",
    "         ) #Add a single hidden layer 2\n",
    "model.add(tf.keras.layers.Dense(units=20, # Number of neurons in hidden layer \n",
    "                                input_shape=(len(features),), \n",
    "                                activation=\"ReLU\")\n",
    "         ) #Add a single hidden layer 3\n",
    "model.add(tf.keras.layers.Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "optimize = tf.keras.optimizers.Adam(learning_rate=0.001) # Adam optimizer with a learning rate of 0.001\n",
    "loss_func = tf.keras.losses.MeanSquaredError() # choosing MSE\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer=optimize,loss=loss_func) \n",
    "\n",
    "X_ = complete_data[features]\n",
    "y_ = complete_data[masses.columns]\n",
    "\n",
    "results = model.fit(X_, y_, epochs=10, batch_size=256, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3lUeX2dDk4S"
   },
   "source": [
    "Clearly, adding more layers helps improve the quality of the network. There is a limit to how effective this is though. Try having 5 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "3kKOzBCrDk4S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "113/113 [==============================] - 1s 2ms/step - loss: 0.0555 - val_loss: 0.0507\n",
      "Epoch 2/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0503 - val_loss: 0.0500\n",
      "Epoch 3/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0499 - val_loss: 0.0499\n",
      "Epoch 4/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0499 - val_loss: 0.0498\n",
      "Epoch 5/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0498 - val_loss: 0.0498\n",
      "Epoch 6/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0498 - val_loss: 0.0498\n",
      "Epoch 7/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0498 - val_loss: 0.0498\n",
      "Epoch 8/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0498 - val_loss: 0.0498\n",
      "Epoch 9/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0498 - val_loss: 0.0498\n",
      "Epoch 10/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0498 - val_loss: 0.0498\n"
     ]
    }
   ],
   "source": [
    "#Complete me: COMPLETE MODEL\n",
    "model = tf.keras.Sequential() #Define the model object\n",
    "model.add(tf.keras.layers.Dense(units=20, # Number of neurons in hidden layer \n",
    "                                input_shape=(len(features),), \n",
    "                                activation=\"ReLU\")\n",
    "         ) #Add a single hidden layer 1\n",
    "model.add(tf.keras.layers.Dense(units=20, # Number of neurons in hidden layer \n",
    "                                input_shape=(len(features),), \n",
    "                                activation=\"ReLU\")\n",
    "         ) #Add a single hidden layer 2\n",
    "model.add(tf.keras.layers.Dense(units=20, # Number of neurons in hidden layer \n",
    "                                input_shape=(len(features),), \n",
    "                                activation=\"ReLU\")\n",
    "         ) #Add a single hidden layer 3\n",
    "model.add(tf.keras.layers.Dense(units=20, # Number of neurons in hidden layer \n",
    "                                input_shape=(len(features),), \n",
    "                                activation=\"ReLU\")\n",
    "         ) #Add a single hidden layer 4\n",
    "model.add(tf.keras.layers.Dense(units=20, # Number of neurons in hidden layer \n",
    "                                input_shape=(len(features),), \n",
    "                                activation=\"ReLU\")\n",
    "         ) #Add a single hidden layer 5\n",
    "model.add(tf.keras.layers.Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "optimize = tf.keras.optimizers.Adam(learning_rate=0.001) # Adam optimizer with a learning rate of 0.001\n",
    "loss_func = tf.keras.losses.MeanSquaredError() # choosing MSE\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer=optimize,loss=loss_func) \n",
    "\n",
    "X_ = complete_data[features]\n",
    "y_ = complete_data[masses.columns]\n",
    "\n",
    "results = model.fit(X_, y_, epochs=10, batch_size=256, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psJeOPOsDk4S"
   },
   "source": [
    "Now, see what happens when you increase the number of neurons per layer from 20 to 50 in the 3 hidden layer model. Consider how they perform compared to ReLU now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "k430N-vIDk4T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "113/113 [==============================] - 1s 2ms/step - loss: 0.1676 - val_loss: 0.0558\n",
      "Epoch 2/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0538 - val_loss: 0.0527\n",
      "Epoch 3/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0520 - val_loss: 0.0516\n",
      "Epoch 4/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0512 - val_loss: 0.0510\n",
      "Epoch 5/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0508 - val_loss: 0.0507\n",
      "Epoch 6/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0505 - val_loss: 0.0505\n",
      "Epoch 7/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0504 - val_loss: 0.0503\n",
      "Epoch 8/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0502 - val_loss: 0.0502\n",
      "Epoch 9/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0502 - val_loss: 0.0501\n",
      "Epoch 10/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0501 - val_loss: 0.0501\n"
     ]
    }
   ],
   "source": [
    "#Complete me:\n",
    "#Complete me: COMPLETE MODEL\n",
    "model = tf.keras.Sequential() #Define the model object\n",
    "model.add(tf.keras.layers.Dense(units=20, # Number of neurons in hidden layer \n",
    "                                input_shape=(len(features),), \n",
    "                                activation=\"ReLU\")\n",
    "         ) #Add a single hidden layer 1\n",
    "model.add(tf.keras.layers.Dense(units=35, # Number of neurons in hidden layer \n",
    "                                input_shape=(len(features),), \n",
    "                                activation=\"ReLU\")\n",
    "         ) #Add a single hidden layer 2\n",
    "model.add(tf.keras.layers.Dense(units=50, # Number of neurons in hidden layer \n",
    "                                input_shape=(len(features),), \n",
    "                                activation=\"ReLU\")\n",
    "         ) #Add a single hidden layer 3\n",
    "model.add(tf.keras.layers.Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "optimize = tf.keras.optimizers.Adam(learning_rate=0.001) # Adam optimizer with a learning rate of 0.001\n",
    "loss_func = tf.keras.losses.MeanSquaredError() # choosing MSE\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer=optimize,loss=loss_func) \n",
    "\n",
    "X_ = complete_data[features]\n",
    "y_ = complete_data[masses.columns]\n",
    "\n",
    "results = model.fit(X_, y_, epochs=10, batch_size=256, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ntl38Zo7Dk4U"
   },
   "source": [
    "Try using the sigmoid and the tanh activation functions again and compare them to ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "JHn1MKIiDk4V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "113/113 [==============================] - 1s 2ms/step - loss: 0.0616 - val_loss: 0.0506\n",
      "Epoch 2/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0502 - val_loss: 0.0500\n",
      "Epoch 3/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0500 - val_loss: 0.0499\n",
      "Epoch 4/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0499 - val_loss: 0.0499\n",
      "Epoch 5/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0498 - val_loss: 0.0498\n",
      "Epoch 6/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0498 - val_loss: 0.0498\n",
      "Epoch 7/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0498 - val_loss: 0.0498\n",
      "Epoch 8/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0498 - val_loss: 0.0498\n",
      "Epoch 9/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0498 - val_loss: 0.0498\n",
      "Epoch 10/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0498 - val_loss: 0.0498\n"
     ]
    }
   ],
   "source": [
    "#Complete me:\n",
    "#Complete me:\n",
    "#Complete me: COMPLETE MODEL\n",
    "model = tf.keras.Sequential() #Define the model object\n",
    "model.add(tf.keras.layers.Dense(units=20, # Number of neurons in hidden layer \n",
    "                                input_shape=(len(features),), \n",
    "                                activation=\"tanh\")\n",
    "         ) #Add a single hidden layer 1\n",
    "model.add(tf.keras.layers.Dense(units=35, # Number of neurons in hidden layer \n",
    "                                input_shape=(len(features),), \n",
    "                                activation=\"tanh\")\n",
    "         ) #Add a single hidden layer 2\n",
    "model.add(tf.keras.layers.Dense(units=50, # Number of neurons in hidden layer \n",
    "                                input_shape=(len(features),), \n",
    "                                activation=\"tanh\")\n",
    "         ) #Add a single hidden layer 3\n",
    "model.add(tf.keras.layers.Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "optimize = tf.keras.optimizers.Adam(learning_rate=0.001) # Adam optimizer with a learning rate of 0.001\n",
    "loss_func = tf.keras.losses.MeanSquaredError() # choosing MSE\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer=optimize,loss=loss_func) \n",
    "\n",
    "X_ = complete_data[features]\n",
    "y_ = complete_data[masses.columns]\n",
    "\n",
    "results = model.fit(X_, y_, epochs=10, batch_size=256, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "113/113 [==============================] - 1s 2ms/step - loss: 0.0499 - val_loss: 0.0498\n",
      "Epoch 2/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0498 - val_loss: 0.0498\n",
      "Epoch 3/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0498 - val_loss: 0.0498\n",
      "Epoch 4/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0498 - val_loss: 0.0498\n",
      "Epoch 5/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0498 - val_loss: 0.0498\n",
      "Epoch 6/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0498 - val_loss: 0.0498\n",
      "Epoch 7/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0498 - val_loss: 0.0498\n",
      "Epoch 8/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0498 - val_loss: 0.0498\n",
      "Epoch 9/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0498 - val_loss: 0.0498\n",
      "Epoch 10/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0498 - val_loss: 0.0498\n"
     ]
    }
   ],
   "source": [
    "#Complete me:\n",
    "#Complete me:\n",
    "#Complete me: COMPLETE MODEL\n",
    "model = tf.keras.Sequential() #Define the model object\n",
    "model.add(tf.keras.layers.Dense(units=20, # Number of neurons in hidden layer \n",
    "                                input_shape=(len(features),), \n",
    "                                activation=\"sigmoid\")\n",
    "         ) #Add a single hidden layer 1\n",
    "model.add(tf.keras.layers.Dense(units=35, # Number of neurons in hidden layer \n",
    "                                input_shape=(len(features),), \n",
    "                                activation=\"sigmoid\")\n",
    "         ) #Add a single hidden layer 2\n",
    "model.add(tf.keras.layers.Dense(units=50, # Number of neurons in hidden layer \n",
    "                                input_shape=(len(features),), \n",
    "                                activation=\"sigmoid\")\n",
    "         ) #Add a single hidden layer 3\n",
    "model.add(tf.keras.layers.Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "optimize = tf.keras.optimizers.Adam(learning_rate=0.001) # Adam optimizer with a learning rate of 0.001\n",
    "loss_func = tf.keras.losses.MeanSquaredError() # choosing MSE\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer=optimize,loss=loss_func) \n",
    "\n",
    "X_ = complete_data[features]\n",
    "y_ = complete_data[masses.columns]\n",
    "\n",
    "results = model.fit(X_, y_, epochs=10, batch_size=256, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-HNQQhpDk4V"
   },
   "source": [
    "This difference in performance, especially with the sigmoid function, is known as the vanishing gradient problem. If the value for any one the neurons gets too far away from 0, the gradient for sigmoid and tanh gets really close to 0. This means that **for deeper networks it is much more difficult to update the weights in the earlier layers as their gradient is so small**. Now, remove the fifth column from the input data, the charge, and see what happens when training. Why do you think including charge has such a large impact, even though it is not required to compute invariant mass?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "ceRTx4CpDk4W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "113/113 [==============================] - 1s 2ms/step - loss: 0.1326 - val_loss: 0.0584\n",
      "Epoch 2/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0550 - val_loss: 0.0530\n",
      "Epoch 3/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0522 - val_loss: 0.0516\n",
      "Epoch 4/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0513 - val_loss: 0.0510\n",
      "Epoch 5/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0509 - val_loss: 0.0507\n",
      "Epoch 6/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0506 - val_loss: 0.0505\n",
      "Epoch 7/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0504 - val_loss: 0.0503\n",
      "Epoch 8/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0503 - val_loss: 0.0502\n",
      "Epoch 9/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0502 - val_loss: 0.0502\n",
      "Epoch 10/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0501 - val_loss: 0.0501\n"
     ]
    }
   ],
   "source": [
    "#Complete me:\n",
    "features_ = [i for i in features if not i.startswith('q')]\n",
    "\n",
    "\n",
    "#Complete me:\n",
    "#Complete me:\n",
    "#Complete me: COMPLETE MODEL\n",
    "model = tf.keras.Sequential() #Define the model object\n",
    "model.add(tf.keras.layers.Dense(units=20, # Number of neurons in hidden layer \n",
    "                                input_shape=(len(features_),), \n",
    "                                activation=\"ReLU\")\n",
    "         ) #Add a single hidden layer 1\n",
    "model.add(tf.keras.layers.Dense(units=35, # Number of neurons in hidden layer \n",
    "                                input_shape=(len(features_),), \n",
    "                                activation=\"ReLU\")\n",
    "         ) #Add a single hidden layer 2\n",
    "model.add(tf.keras.layers.Dense(units=50, # Number of neurons in hidden layer \n",
    "                                input_shape=(len(features_),), \n",
    "                                activation=\"ReLU\")\n",
    "         ) #Add a single hidden layer 3\n",
    "model.add(tf.keras.layers.Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "optimize = tf.keras.optimizers.Adam(learning_rate=0.001) # Adam optimizer with a learning rate of 0.001\n",
    "loss_func = tf.keras.losses.MeanSquaredError() # choosing MSE\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer=optimize,loss=loss_func) \n",
    "\n",
    "X_ = complete_data[features_]\n",
    "y_ = complete_data[masses.columns]\n",
    "\n",
    "results = model.fit(X_, y_, epochs=10, batch_size=256, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IscwBDy7Dk4W"
   },
   "source": [
    "Finally, there are other options for the loss function. Try experimenting with alternatives to mean squared error.\n",
    "\n",
    "<https://www.tensorflow.org/api_docs/python/tf/keras/losses>\n",
    "\n",
    "You can also try some other optimizers -- for example, sgd (with and without momentum), rmsprop, adagrad, adadelta, adamax, and nadam. <https://www.tensorflow.org/api_docs/python/tf/keras/optimizers>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCm5VSXDDk4X"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
