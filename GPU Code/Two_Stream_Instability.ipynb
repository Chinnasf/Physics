{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kchinas/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scienceplots\n",
    "import imageio.v2 as imageio\n",
    "import glob\n",
    "\n",
    "from matplotlib import animation\n",
    "from matplotlib.animation import PillowWriter\n",
    "\n",
    "plt.style.use(['science', 'notebook'])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TWO STREAM INSTABILITY USING TORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L  = 100    # Domain of the solution 0 <= x <= L  (in Debye lengths)\n",
    "N  = 25000  # Number of electrons\n",
    "J  = 1000   # Number of grid-points\n",
    "vb = 5      # Beam velocity\n",
    "n0 = N/L    # ion number density\n",
    "dx = L/J\n",
    "\n",
    "dt = 0.1    # time step  (in inverse plasma frequencies)\n",
    "t_max = 150  # such that 0 <= t <= t_max\n",
    "timesteps = int(t_max / dt)\n",
    "\n",
    "\n",
    "# Check input parameters make sence:\n",
    "if (N < 1) | (J < 2) | (L <= 0.) | (vb <= 0.) | (dt <= 0.) | (t_max <= 0.) | ((int (t_max / dt) / 10) < 1):\n",
    "    print(\"Error - invalid input parameters\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Goal                     | NumPy                                | PyTorch                                |\n",
    "|--------------------------|---------------------------------------|----------------------------------------|\n",
    "| Uniform sample [a, b)    | `np.random.uniform(a, b, size)`       | `(b - a) * torch.rand(size) + a`       |\n",
    "| Normal sample (mean, σ)  | `np.random.normal(mean, std, size)`   | `torch.normal(mean, std, size)`        |\n",
    "| Random ints [a, b)       | `np.random.randint(a, b, size)`       | `torch.randint(a, b, size)`            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code takes a lot of time to run, despite it is the torch version of the numpy implementation. \n",
    "\n",
    "```Python\n",
    "\n",
    "# Initial positions\n",
    "r0 = torch.rand(N, device=device) * L\n",
    "\n",
    "# Initial velocities with rejection sampling\n",
    "def sample_velocity(v_b, tails_factor):\n",
    "    f_max = 0.5 * (1.0 + np.exp(-2 * v_b**2))  # ok to use np.exp since v_b is a scalar\n",
    "    vmin, vmax = -tails_factor * v_b, tails_factor * v_b\n",
    "\n",
    "    velocities = torch.empty(N, device=device)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        v_ = (vmax - vmin) * torch.rand(1, device=device) + vmin  # torch version\n",
    "        beam_shape = 0.5 * (torch.exp(-0.5 * (v_ - v_b)**2) + torch.exp(-0.5 * (v_ + v_b)**2))\n",
    "        gamma = torch.rand(1, device=device) * f_max\n",
    "        if gamma <= beam_shape:\n",
    "            velocities[i] = v_\n",
    "            i += 1\n",
    "    return velocities\n",
    "\n",
    "v0 = sample_velocity(vb, 4)\n",
    "```\n",
    "\n",
    "This is because NumPy is highly optimized for scalar and small-batch CPU operations, and its core is implemented in C — so for modest sizes like N = 25,000, rejection sampling in NumPy can outperform PyTorch if you're not using vectorized operations. Thus, it's simpler to implementin numpy and then convert. \n",
    "\n",
    "### USING BATCHES\n",
    "\n",
    "However, the original rejection sampling does one sample at a time — **very inefficient**, especially on a GPU where batch operations are massively faster.\n",
    "\n",
    "By generating many random samples in batches, we:\n",
    "\n",
    "1. Reduce the number of iterations in the while loop.\n",
    "\n",
    "2. Allow operations like `torch.exp`, `torch.rand`, and masking to be applied in parallel over large arrays.\n",
    "\n",
    "3. **Use the GPU (or SIMD instructions on CPU) as they were designed to be used** — on big chunks of data.\n",
    "\n",
    "So batch_size controls how many samples we generate at once, hoping that many of them will be accepted in a single round.\n",
    "\n",
    "**WHICH BATCH SIZE?** This is a heuristic value, for a real-world application or a more serious scenario, it is recommended to work on a benchmark script to test different batch sizes and chose the optimal value. For now, it will be set `batch_size = 100000` as a heuristic value: big enough to efficiently use the GPU, small enough to avoid memory overflow.\n",
    "\n",
    "\n",
    "| Scenario                   | NumPy (loop)    | Torch (vectorized)            |\n",
    "| -------------------------- | --------------- | ----------------------------- |\n",
    "| Small N, CPU-only          | Probably faster | Slightly slower               |\n",
    "| Large N (e.g., 100k+), CPU | About equal     | Faster with batching          |\n",
    "| Large N, **GPU** available | ❌ CPU-bound     | ✅ Hugely faster with batching |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial positions\n",
    "r0 = torch.rand(N, device=device) * L\n",
    "\n",
    "def sample_velocity_vectorized(v_b, tails_factor, N, batch_size=100000):\n",
    "    f_max = 0.5 * (1.0 + np.exp(-2 * v_b**2))\n",
    "    vmin, vmax = -tails_factor * v_b, tails_factor * v_b\n",
    "\n",
    "    velocities = torch.empty(N, device=device)\n",
    "    filled = 0\n",
    "\n",
    "    while filled < N:\n",
    "        v_ = (vmax - vmin) * torch.rand(batch_size, device=device) + vmin\n",
    "        beam_shape = 0.5 * (torch.exp(-0.5 * (v_ - v_b)**2) + torch.exp(-0.5 * (v_ + v_b)**2))\n",
    "        gamma = torch.rand(batch_size, device=device) * f_max\n",
    "        accepted = v_[gamma <= beam_shape]\n",
    "\n",
    "        num_to_fill = min(accepted.shape[0], N - filled)\n",
    "        velocities[filled:filled+num_to_fill] = accepted[:num_to_fill]\n",
    "        filled += num_to_fill\n",
    "\n",
    "    return velocities\n",
    "v0 = sample_velocity_vectorized(vb,4)\n",
    "velocity_tags = abs(v0 - vb) < abs(v0 + vb) # False = left going"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_optimization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
