{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scienceplots\n",
    "import imageio.v2 as imageio\n",
    "import glob\n",
    "\n",
    "from matplotlib import animation\n",
    "from matplotlib.animation import PillowWriter\n",
    "\n",
    "plt.style.use(['science', 'notebook'])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TWO STREAM INSTABILITY USING TORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "L  = 100    # Domain of the solution 0 <= x <= L  (in Debye lengths)\n",
    "N  = 25000  # Number of electrons\n",
    "J  = 1000   # Number of grid-points\n",
    "vb = 5      # Beam velocity\n",
    "n0 = N/L    # ion number density\n",
    "dx = L/J\n",
    "\n",
    "dt = 0.1    # time step  (in inverse plasma frequencies)\n",
    "t_max = 150  # such that 0 <= t <= t_max\n",
    "timesteps = int(t_max / dt)\n",
    "\n",
    "\n",
    "# Check input parameters make sence:\n",
    "if (N < 1) | (J < 2) | (L <= 0.) | (vb <= 0.) | (dt <= 0.) | (t_max <= 0.) | ((int (t_max / dt) / 10) < 1):\n",
    "    print(\"Error - invalid input parameters\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Goal                     | NumPy                                | PyTorch                                |\n",
    "|--------------------------|---------------------------------------|----------------------------------------|\n",
    "| Uniform sample [a, b)    | `np.random.uniform(a, b, size)`       | `(b - a) * torch.rand(size) + a`       |\n",
    "| Normal sample (mean, σ)  | `np.random.normal(mean, std, size)`   | `torch.normal(mean, std, size)`        |\n",
    "| Random ints [a, b)       | `np.random.randint(a, b, size)`       | `torch.randint(a, b, size)`            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code takes a lot of time to run, despite it is the torch version of the numpy implementation. \n",
    "\n",
    "```Python\n",
    "\n",
    "# Initial positions\n",
    "r0 = torch.rand(N, device=device) * L\n",
    "\n",
    "# Initial velocities with rejection sampling\n",
    "def sample_velocity(v_b, tails_factor):\n",
    "    f_max = 0.5 * (1.0 + np.exp(-2 * v_b**2))  # ok to use np.exp since v_b is a scalar\n",
    "    vmin, vmax = -tails_factor * v_b, tails_factor * v_b\n",
    "\n",
    "    velocities = torch.empty(N, device=device)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        v_ = (vmax - vmin) * torch.rand(1, device=device) + vmin  # torch version\n",
    "        beam_shape = 0.5 * (torch.exp(-0.5 * (v_ - v_b)**2) + torch.exp(-0.5 * (v_ + v_b)**2))\n",
    "        gamma = torch.rand(1, device=device) * f_max\n",
    "        if gamma <= beam_shape:\n",
    "            velocities[i] = v_\n",
    "            i += 1\n",
    "    return velocities\n",
    "\n",
    "v0 = sample_velocity(vb, 4)\n",
    "```\n",
    "\n",
    "This is because NumPy is highly optimized for scalar and small-batch CPU operations, and its core is implemented in C — so for modest sizes like N = 25,000, rejection sampling in NumPy can outperform PyTorch if you're not using vectorized operations. Thus, it's simpler to implementin numpy and then convert. \n",
    "\n",
    "### USING BATCHES\n",
    "\n",
    "However, the original rejection sampling does one sample at a time — **very inefficient**, especially on a GPU where batch operations are massively faster.\n",
    "\n",
    "By generating many random samples in batches, we:\n",
    "\n",
    "1. Reduce the number of iterations in the while loop.\n",
    "\n",
    "2. Allow operations like `torch.exp`, `torch.rand`, and masking to be applied in parallel over large arrays.\n",
    "\n",
    "3. **Use the GPU (or SIMD instructions on CPU) as they were designed to be used** — on big chunks of data.\n",
    "\n",
    "So batch_size controls how many samples we generate at once, hoping that many of them will be accepted in a single round.\n",
    "\n",
    "**WHICH BATCH SIZE?** This is a heuristic value, for a real-world application or a more serious scenario, it is recommended to work on a benchmark script to test different batch sizes and chose the optimal value. For now, it will be set `batch_size = 100000` as a heuristic value: big enough to efficiently use the GPU, small enough to avoid memory overflow.\n",
    "\n",
    "\n",
    "| Scenario                   | NumPy (loop)    | Torch (vectorized)            |\n",
    "| -------------------------- | --------------- | ----------------------------- |\n",
    "| Small N, CPU-only          | Probably faster | Slightly slower               |\n",
    "| Large N (e.g., 100k+), CPU | About equal     | Faster with batching          |\n",
    "| Large N, **GPU** available | ❌ CPU-bound     | ✅ Hugely faster with batching |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial positions\n",
    "r0 = torch.rand(N, device=device) * L\n",
    "\n",
    "def sample_velocity(v_b, tails_factor, batch_size=100000):\n",
    "    f_max = 0.5 * (1.0 + np.exp(-2 * v_b**2))\n",
    "    vmin, vmax = -tails_factor * v_b, tails_factor * v_b\n",
    "\n",
    "    velocities = torch.empty(N, device=device)\n",
    "    filled = 0\n",
    "\n",
    "    while filled < N:\n",
    "        v_ = (vmax - vmin) * torch.rand(batch_size, device=device) + vmin\n",
    "        beam_shape = 0.5 * (torch.exp(-0.5 * (v_ - v_b)**2) + torch.exp(-0.5 * (v_ + v_b)**2))\n",
    "        gamma = torch.rand(batch_size, device=device) * f_max\n",
    "        accepted = v_[gamma <= beam_shape]\n",
    "\n",
    "        num_to_fill = min(accepted.shape[0], N - filled)\n",
    "        velocities[filled:filled+num_to_fill] = accepted[:num_to_fill]\n",
    "        filled += num_to_fill\n",
    "\n",
    "    return velocities\n",
    "v0 = sample_velocity(vb,4)\n",
    "velocity_tags = abs(v0 - vb) < abs(v0 + vb) # False = left going"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scatter_add_` handles repeated indices like `np.add.at`; It's fast, **parallel**, a\n",
    "\n",
    "Next, we will use `torch.zeros` instead of `torch.empty`, why? Because we are doing an addition of the previous values. \n",
    "\n",
    "\n",
    "| Function      | Use case                                                                    |\n",
    "| ------------- | --------------------------------------------------------------------------- |\n",
    "| `torch.empty` | You're **immediately overwriting** all values (e.g., `velocities[:] = ...`) |\n",
    "| `torch.zeros` | You're **accumulating**, or using values **before** assigning all of them   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_density(r_):\n",
    "    ne_ = torch.zeros(J, device=device)\n",
    "    j_  = torch.floor(r_ / dx).long()\n",
    "    f   = (r_ / dx) - j_ # fractional offset\n",
    "    \n",
    "    # Right: weight = f / dx\n",
    "    ne_.scatter_add_(0, (j_+1)%J, f / dx)\n",
    "    \n",
    "    # Left: weight = (1 - f) / dx\n",
    "    ne_.scatter_add_(0, j_%J, (1 - f) / dx)\n",
    "    \n",
    "    return ne_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: Poisson solver. \n",
    "\n",
    "The existing NumPy version uses np.fft.fft and np.fft.ifft, which are efficient and totally valid.\n",
    "\n",
    "There are two options:\n",
    "\n",
    "| Option | Approach            | Use it if...                                         |\n",
    "| ------ | ------------------- | ---------------------------------------------------- |\n",
    "| ✅ 1    | **Keep NumPy FFTs** | You're OK with a NumPy dependency, or not on GPU yet |\n",
    "| ⚡ 2    | `torch.fft.fft`     | You want full GPU usage (no CPU↔GPU transfers)       |\n",
    "\n",
    "\n",
    "Option 2: \n",
    "* Uses `torch.fft.fft`, `ifft`, and `fftfreq` equivalents.\n",
    "* All operations stay on `device`, so no CPU-GPU ping-pong.\n",
    "* Just like in `NumPy`, we set `phi_hat[0] = 0` to avoid the DC offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_solver(ne_):\n",
    "    rho = ne_ / n0 - 1 # normalized charge density\n",
    "    \n",
    "    # FFT of rho (torch.fft returns complex tensor)\n",
    "    rho_hat = torch.fft.fft(rho)\n",
    "    \n",
    "    # Wavenumbers (same as np.fft.fftfreq)\n",
    "    k = 2 * np.pi * torch.fft.fftfreq(J, d=L/J).to(device)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    phi_hat = torch.zeros_like(rho_hat)\n",
    "    phi_hat[1:] = -rho_hat[1:] / (k[1:]**2)\n",
    "    phi_hat[0] = 0.0 + 0.0j\n",
    "    \n",
    "    # Inverse FFT to get potential phi(x)\n",
    "    phi = torch.fft.ifft(phi_hat).real  # result should be real-valued\n",
    "    \n",
    "    return phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Electric field $\\rightarrow$ getting rid of the `for` loop.\n",
    "\n",
    "For this, we will be using `torch.roll`. Why?\n",
    "* `torch.roll` is the perfect tool for periodic boundaries (% J becomes unnecessary)\n",
    "* Vectorized, GPU-ready, and avoids loops.\n",
    "\n",
    "`torch.roll(input, shifts, dims=None)` → Tensor\n",
    "\n",
    "Roll the tensor `input` along the given dimension(s). Elements that are shifted beyond the last position are re-introduced at the first position. If dims is None, the tensor will be flattened before rolling and then restored to the original shape.\n",
    "\n",
    "Example \n",
    "\n",
    "```Python\n",
    ">>> x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]).view(4, 2)\n",
    ">>> x\n",
    "tensor([[1, 2],\n",
    "        [3, 4],\n",
    "        [5, 6],\n",
    "        [7, 8]])\n",
    ">>> torch.roll(x, 1)\n",
    "tensor([[8, 1],\n",
    "        [2, 3],\n",
    "        [4, 5],\n",
    "        [6, 7]])\n",
    ">>> torch.roll(x, 1, 0)\n",
    "tensor([[7, 8],\n",
    "        [1, 2],\n",
    "        [3, 4],\n",
    "        [5, 6]])\n",
    ">>> torch.roll(x, -1, 0)\n",
    "tensor([[3, 4],\n",
    "        [5, 6],\n",
    "        [7, 8],\n",
    "        [1, 2]])\n",
    ">>> torch.roll(x, shifts=(2, 1), dims=(0, 1))\n",
    "tensor([[6, 5],\n",
    "        [8, 7],\n",
    "        [2, 1],\n",
    "        [4, 3]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def electric_field(phi_):\n",
    "    # Use periodic shifting\n",
    "    phi_left  = torch.roll(phi_, shifts=1, dims=0)\n",
    "    phi_right = torch.roll(phi_, shifts=-1, dims=0)\n",
    "    \n",
    "    E_grid = (phi_left - phi_right) / (2 * dx)\n",
    "    return E_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the electric field perceived by the particles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def particle_electric_field(r_, E_):\n",
    "    j_ = torch.floor(r_ / dx).long()\n",
    "    f_ = (r_ / dx) - j_\n",
    "\n",
    "    E_particles = (1 - f_)*E_[ j_ % J ] + f_*E_[ (j_ + 1) % J ]\n",
    "    return E_particles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us calculate the derivatives from the equations of motion to be fed as functions for the Runge-Kutta solutions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_r_v(y_):\n",
    "    \"\"\"\n",
    "    Returns dy/dt, with f = [r0,r1, ..., rN,v0,v1,...,vN]\n",
    "    Such that dy/dt = [dr/dt, dv/dt] = [v(r_i), -E(r_i)]\n",
    "    For notation dy/dt --> dy [tuple]\n",
    "    \"\"\"\n",
    "    \n",
    "    r_ = y_[:N]\n",
    "    v_ = y_[N:] \n",
    "    \n",
    "    # 1. Compute the density\n",
    "    ne = compute_density(r_)\n",
    "    \n",
    "    # 2. Solve Poisson's equation\n",
    "    phi = poisson_solver(ne)\n",
    "    \n",
    "    # 3. Compute the Electric field in the grid\n",
    "    E_grid = electric_field(phi)\n",
    "    \n",
    "    # 4. Compute the Electric field per particle\n",
    "    E_particles = particle_electric_field(r_,E_grid)\n",
    "    \n",
    "    return torch.cat([v_, -E_particles])\n",
    "\n",
    "\n",
    "def RungeKutta4(y_):\n",
    "    \"\"\"\n",
    "    dy = dy/dt = [dr/dt, dv/dt] = [v(r_i), -E(r_i)]\n",
    "    \"\"\"\n",
    "    \n",
    "    # f(x,y)    \n",
    "    k1 = derivative_r_v( y_)\n",
    "    k2 = derivative_r_v( y_ + 0.5*dt*k1)\n",
    "    k3 = derivative_r_v( y_ + 0.5*dt*k2)\n",
    "    k4 = derivative_r_v( y_ + dt*k3)\n",
    "    \n",
    "    return y_ + (dt / 6) * (k1 + 2*k2 + 2*k3 + k4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.cat([r0, v0])\n",
    "for step in range(timesteps):\n",
    "    y = RungeKutta4(y)\n",
    "    # Enforce periodic boundary conditions\n",
    "    y[:N] = y[:N] % L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_optimization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
